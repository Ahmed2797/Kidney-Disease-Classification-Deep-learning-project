{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deed9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b20cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ahmed/project/Kidney-Disease-Classification-Deep-learning-project/recsearch'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd216f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da684b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ahmed/project/Kidney-Disease-Classification-Deep-learning-project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d1ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5fa1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    \"\"\"\n",
    "    Configuration dataclass for model evaluation stage.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): Root directory for storing evaluation artifacts.\n",
    "        report_file_path (Path): Path to save the main evaluation report YAML/JSON.\n",
    "        threshold_accuracy (float): Minimum accuracy threshold to consider model acceptable.\n",
    "        scores_file_dir (Path): Directory to store evaluation scores.\n",
    "        scores_file (str): Filename for the evaluation scores JSON.\n",
    "        report_file_dir (Path): Directory to store detailed evaluation reports.\n",
    "        report_file (str): Filename for the evaluation report JSON.\n",
    "        mlflow_tracking_uri (str): MLflow tracking server URI for logging metrics and models.\n",
    "        mlflow_experiment_name (str): Name of the MLflow experiment.\n",
    "        all_params (Dict): Dictionary of all hyperparameters and config values for reference/logging.\n",
    "        param_image_size (List[int]): Image size used during training/evaluation (Height, Width, Channels).\n",
    "        param_batch_size (int): Batch size used during evaluation.\n",
    "        training_data_path (Path): Path to the validation dataset for model evaluation.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    report_file_path: Path\n",
    "    report_file_dir: Path\n",
    "    report_file: str\n",
    "    training_data_path: Path\n",
    "\n",
    "    scores_file_dir: Path\n",
    "    scores_file: str\n",
    "\n",
    "    mlflow_tracking_uri: str\n",
    "    mlflow_experiment_name: str\n",
    "\n",
    "    all_params: dict\n",
    "    param_image_size: list\n",
    "    param_batch_size: int\n",
    "    threshold_accuracy: float \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254728b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.constants import *\n",
    "from project.utils import create_directories,read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54a9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigerationManager:\n",
    "    \"\"\"\n",
    "    Manages the loading and parsing of configuration and parameter YAML files.\n",
    "    Creates required directories and provides configuration objects \n",
    "    for different pipeline components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=CONFIG_YAML_FILE, param=PARAM_YAML_FILE):\n",
    "        \"\"\"\n",
    "        Initialize the Configuration Manager.\n",
    "\n",
    "        Args:\n",
    "            config (str): Path to the main configuration YAML file.\n",
    "            param (str): Path to the parameters YAML file.\n",
    "\n",
    "        Raises:\n",
    "            CustomException: If YAML reading or directory creation fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.config = read_yaml(config)\n",
    "            self.param = read_yaml(param)\n",
    "            create_directories(self.config.artifacts_root)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in ConfigurationManager initialization: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \"\"\"\n",
    "        Create and return the ModelEvaluationConfig dataclass.\n",
    "\n",
    "        This method:\n",
    "            1. Reads the model evaluation section from the main config.\n",
    "            2. Ensures all required directories exist.\n",
    "            3. Converts string paths to Path objects for OS-independent handling.\n",
    "            4. Returns a fully populated ModelEvaluationConfig object.\n",
    "        \n",
    "        Returns:\n",
    "            ModelEvaluationConfig: Dataclass containing paths, MLflow info, params, \n",
    "                                and evaluation-specific settings.\n",
    "        \"\"\"\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        # Ensure directories exist\n",
    "        create_directories([\n",
    "            Path(config.root_dir),\n",
    "            Path(config.scores_file_dir),\n",
    "            Path(config.report_file_dir)\n",
    "        ])\n",
    "\n",
    "        # Build evaluation config\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            report_file_path=Path(config.report_file_path),\n",
    "            threshold_accuracy=config.threshold_accuracy,\n",
    "            scores_file_dir=Path(config.scores_file_dir),\n",
    "            scores_file=config.scores_file,\n",
    "            report_file_dir=Path(config.report_file_dir),\n",
    "            report_file=config.report_file,\n",
    "            mlflow_tracking_uri=config.mlflow_tracking_uri,\n",
    "            mlflow_experiment_name=config.mlflow_experiment_name,\n",
    "            all_params=self.param.to_dict(),\n",
    "            param_image_size=self.param.IMAGE_SIZE,\n",
    "            param_batch_size=self.param.BATCH_SIZE,\n",
    "            training_data_path=Path(\n",
    "                self.config.data_ingestion.unzip_dir\n",
    "            ) / \"kidney-ct-scan-image\"\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f690b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "#sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\")))\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "from project.entity.config import ModelEvaluationConfig\n",
    "from project.utils import save_json\n",
    "\n",
    "class Evaluation:\n",
    "    \"\"\"\n",
    "    Handles model evaluation, saving scores, and logging metrics/models to MLflow.\n",
    "\n",
    "    Args:\n",
    "        config (ModelEvaluationConfig): Configuration object for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def _valid_generator(self):\n",
    "        \"\"\"Create validation dataset using tf.data with proper preprocessing.\"\"\"\n",
    "        img_size = tuple(self.config.param_image_size[:-1])  # (H, W)\n",
    "        batch_size = self.config.param_batch_size\n",
    "\n",
    "        val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            self.config.training_data_path,\n",
    "            validation_split=0.30,\n",
    "            subset=\"validation\",\n",
    "            seed=42,\n",
    "            image_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            label_mode='categorical',\n",
    "        )\n",
    "\n",
    "        normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "        val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        self.valid_generator = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model_path: Path):\n",
    "        \"\"\"Load the trained model from the given path.\"\"\"\n",
    "        return tf.keras.models.load_model(model_path)\n",
    "\n",
    "    def evalution(self):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            results (list): [loss, accuracy]\n",
    "        \"\"\"\n",
    "        self._valid_generator()\n",
    "        model = self.load_model(Path(\"artifacts/training/model.h5\"))\n",
    "        results = model.evaluate(self.valid_generator)\n",
    "        save_json(\n",
    "            path=Path(self.config.report_file_path),\n",
    "            data={\"loss\": results[0], \"accuracy\": results[1]}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def save_score(self):\n",
    "        \"\"\"\n",
    "        Save evaluation scores (loss and accuracy) as JSON.\n",
    "        \"\"\"\n",
    "        results = self.evalution()\n",
    "        scores_file_path = Path(self.config.scores_file_dir) / self.config.scores_file\n",
    "        scores = {\"loss\": results[0], \"accuracy\": results[1]}\n",
    "        save_json(path=scores_file_path, data=scores)\n",
    "\n",
    "    def log_mlflow(self):\n",
    "        \"\"\"\n",
    "        Log evaluation metrics and trained model to MLflow/DAGsHub.\n",
    "        \"\"\"\n",
    "        # Set DAGsHub credentials (securely)\n",
    "        os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"Ahmed2797\"\n",
    "        os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"466cd6e40b4463c19cee521d93d34f35fb915367\"\n",
    "\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_tracking_uri)\n",
    "        mlflow.set_experiment(self.config.mlflow_experiment_name)\n",
    "\n",
    "        # Evaluate model\n",
    "        results = self.evalution()  # [loss, accuracy]\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Log hyperparameters\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"val_loss\", results[0])\n",
    "            mlflow.log_metric(\"val_accuracy\", results[1])\n",
    "\n",
    "            # Log trained model\n",
    "            model = self.load_model(Path(\"artifacts/training/model.h5\"))\n",
    "\n",
    "            # Use export() for SavedModel format\n",
    "            export_dir = Path(\"artifacts/training/model_export\")\n",
    "            export_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model.export(export_dir)  # TensorFlow 2.12+ method\n",
    "\n",
    "            # Log the exported model folder as an artifact\n",
    "            mlflow.log_artifacts(str(export_dir), artifact_path=\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e0a36e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 files belonging to 2 classes.\n",
      "Using 139 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 18:41:09.233318: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-11-26 18:41:11.646591: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 205520896 exceeds 10% of free system memory.\n",
      "2025-11-26 18:41:12.035282: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 205520896 exceeds 10% of free system memory.\n",
      "2025-11-26 18:41:12.498600: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 51380224 exceeds 10% of free system memory.\n",
      "2025-11-26 18:41:12.568387: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 102760448 exceeds 10% of free system memory.\n",
      "2025-11-26 18:41:12.696119: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 102760448 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3s/step - accuracy: 0.7122 - loss: 1.2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/kidney/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/11/26 18:41:36 INFO mlflow.tracking.fluent: Experiment with name 'kidney_classification_experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 files belonging to 2 classes.\n",
      "Using 139 files for validation.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - accuracy: 0.7122 - loss: 1.2180\n",
      "Saved artifact at 'artifacts/training/model_export'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  136002490536336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490536720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490537680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490536912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490538064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490537872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490538448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490538256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490538832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490538640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490539216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490539024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490539600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490539408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490539984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490539792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490540368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490540176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490540752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490540560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490541136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490540944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490541520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490541328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490541904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490541712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490542288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136002490542096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigerationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "\n",
    "    evaluation = Evaluation(config=model_evaluation_config)\n",
    "    evaluation.save_score()\n",
    "    evaluation.log_mlflow()\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error during model evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943fb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kidney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
